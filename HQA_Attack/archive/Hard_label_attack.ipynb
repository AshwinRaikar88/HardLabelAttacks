{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f01c79-18eb-41d7-b3a7-56eba6584815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/raikara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/raikara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/raikara/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from datasets import load_dataset\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c1d822-8f52-403d-b7bf-bd4594c2d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b7a92c-6be5-499d-97fa-cb5575990b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Retrieve a set of synonyms for a given word using WordNet.\n",
    "\n",
    "    word: string representing the target word\n",
    "\n",
    "    return: set of synonyms for the word\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "      for lemma in syn.lemmas():\n",
    "        lemm = lemma.name().replace(\"_\", \" \")\n",
    "        if len(lemm.split()) > 1:\n",
    "          # Multi word synonyms are discarded\n",
    "          continue\n",
    "        synonyms.add(lemm)\n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_synonyms(adversarial_example, pos_tags, target_pos):\n",
    "      \"\"\"\n",
    "      Replace words in an adversarial example with their synonyms based on their\n",
    "      POS tags.\n",
    "\n",
    "      adversarial_example: randomly initialized adverstial example\n",
    "      pos_tags: POS tags\n",
    "      target_pos: set of broad POS categories to be replaced\n",
    "\n",
    "      return: modified adversarial example\n",
    "      \"\"\"\n",
    "      for i, (word, pos) in enumerate(pos_tags):\n",
    "          if pos[:2] in target_pos:  # Match broad POS categories\n",
    "              synonyms = get_synonyms(word)\n",
    "              if synonyms:\n",
    "                  adversarial_example[i] = random.choice(synonyms)  # Random synonym replacement\n",
    "      return ' '.join(adversarial_example)\n",
    "\n",
    "def generate_random_adversarial_example(x, f):\n",
    "    \"\"\"\n",
    "    Generate a random adversarial example by replacing certain words with synonyms.\n",
    "\n",
    "    x: list of words (original text)\n",
    "    f: victim model (a function that returns model predictions)\n",
    "\n",
    "    return: new generated adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    max_iter = 100\n",
    "\n",
    "    # Get Part-Of-Speech (POS) tags for words in x\n",
    "    pos_tags = nltk.pos_tag(x)\n",
    "    adversarial_example = x[:]  # Copy the original text\n",
    "    original_exmp = ' '.join(x)\n",
    "    \n",
    "    # Define POS tags to be replaced (NN: noun, VB: verb, RB: adverb, JJ: adjective)\n",
    "    target_pos = {'NN', 'VB', 'RB', 'JJ'}\n",
    "\n",
    "    adv_exmp = replace_synonyms(adversarial_example, pos_tags, target_pos)\n",
    "\n",
    "    query_count = 1\n",
    "    # Ensure the adversarial condition is met (prediction change)\n",
    "    while f(adv_exmp)[0]['label'] == f(original_exmp)[0]['label']:\n",
    "        if query_count == max_iter:\n",
    "            break\n",
    "        query_count += 1\n",
    "\n",
    "        pos_tags = nltk.pos_tag(adversarial_example)\n",
    "        adv_exmp = replace_synonyms(adversarial_example, pos_tags, target_pos)\n",
    "\n",
    "        if not any(get_synonyms(word) for word, pos in pos_tags if pos[:2] in target_pos):\n",
    "            print(\"No synonyms found!\")\n",
    "            break\n",
    "\n",
    "    if DEBUG:\n",
    "      print(f\"No. of queries made = {query_count}\")\n",
    "      if query_count < max_iter:\n",
    "          print(\"Adversarial example found!\")\n",
    "      else:\n",
    "          print(\"Max iterations reached!\")\n",
    "          print(\"No adversarial example found!\")\n",
    "\n",
    "    return adv_exmp, query_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d97d9f-5f6a-40c7-9269-5e7062f06144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acec8f25-56b3-454d-bec0-8d7133c7410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class US_Encoder:\n",
    "    \"\"\"\n",
    "    A class to hold Universal Sequence Encoder\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):            \n",
    "        self.model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        print(\"Universal Sequence Encoder model loaded!\")\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute the similarity score between two texts using Universal Sentence Encoder (USE).\n",
    "\n",
    "        text1: First sentence (string)\n",
    "        text2: Second sentence (string)\n",
    "\n",
    "        return: Cosine similarity score\n",
    "        \"\"\"\n",
    "        # Encode the sentences into embeddings\n",
    "        embeddings = self.model([text1, text2])\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = np.inner(embeddings[0], embeddings[1])\n",
    "\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb2d54e-16ef-4406-b8ae-764a9898f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sequence Encoder model loaded!\n"
     ]
    }
   ],
   "source": [
    "use_encoder = US_Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3f74fd5-a742-4b22-ba53-3fd1e5abce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.4120\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text1 = \"I enjoy working\"\n",
    "text2 = \"I enjoy working with NLP models.\"\n",
    "\n",
    "similarity_score = use_encoder.compute_similarity(text1, text2)\n",
    "print(f\"Similarity Score: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab190d-48ae-4d94-b0af-010de9f2a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_original_words(x, x_t, f, query_count):\n",
    "    \"\"\"\n",
    "    Improved Algorithm 1: Substituting Original Words Back. New implementation\n",
    "\n",
    "    x: Original text (list of words)\n",
    "    x_t: Adversarial example (list of words)\n",
    "    f: Victim model (a function that returns model predictions)\n",
    "    compute_similarity: Function to compute similarity between sentences\n",
    "    query_count: Counter for model queries\n",
    "\n",
    "    return: New adversarial example x_t after substitution\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        diffs = [i for i, (orig, adv) in enumerate(zip(x, x_t)) if orig != adv]\n",
    "        if not diffs:\n",
    "            print(\"No differences remaining.\")\n",
    "            break\n",
    "\n",
    "        best_choice = None\n",
    "        best_sim_score = -1\n",
    "        best_x_tmp = None\n",
    "\n",
    "        for i in diffs:\n",
    "            x_tmp = copy.deepcopy(x_t)\n",
    "            x_tmp[i] = x[i]  # Replace adversarial word with original\n",
    "\n",
    "            sim_score = compute_similarity(' '.join(x), ' '.join(x_tmp))\n",
    "            \n",
    "            if sim_score > best_sim_score:\n",
    "                best_sim_score = sim_score\n",
    "                best_choice = i\n",
    "                best_x_tmp = x_tmp\n",
    "\n",
    "        if best_choice is not None:\n",
    "            # Check if adversarial condition is still met\n",
    "            if f(' '.join(best_x_tmp))[0]['label'] != f(' '.join(x))[0]['label']:\n",
    "                query_count += 1\n",
    "                x_t = best_x_tmp  # Apply best rollback found\n",
    "            else:\n",
    "                break  # Stop if no more valid replacements can be made\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return x_t, query_count\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
